{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a937a7",
   "metadata": {},
   "source": [
    "# STAT7008 Project: Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4058704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb303a1-a13e-4da9-84fe-e8986dcfb091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/hetrec2011-lastfm-2k.zip\n",
      "  inflating: data/last_fm/user_friends.dat  \n",
      "  inflating: data/last_fm/user_taggedartists.dat  \n",
      "  inflating: data/last_fm/user_taggedartists-timestamps.dat  \n",
      "  inflating: data/last_fm/artists.dat  \n",
      "  inflating: data/last_fm/readme.txt  \n",
      "  inflating: data/last_fm/tags.dat   \n",
      "  inflating: data/last_fm/user_artists.dat  \n",
      "Archive:  data/ml-1m.zip\n",
      "   creating: data/ml-1m/\n",
      "  inflating: data/ml-1m/movies.dat   \n",
      "  inflating: data/ml-1m/ratings.dat  \n",
      "  inflating: data/ml-1m/README       \n",
      "  inflating: data/ml-1m/users.dat    \n"
     ]
    }
   ],
   "source": [
    "!sh scripts/process.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33855e49",
   "metadata": {},
   "source": [
    "### (1)\n",
    "\n",
    "Typical recommendation algorithms: Content-based filtering, Item-based collaborative filtering, and User-based collaborative filtering\n",
    "\n",
    "Data Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7a481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"data/ml-1m/\"\n",
    "\n",
    "# Load Movies\n",
    "movies = pd.read_csv(datapath + 'movies.dat', delimiter='::', engine='python', header=None, names=['MovieID', 'Title', 'Genres'], encoding='latin-1')\n",
    "\n",
    "# Load Ratings\n",
    "ratings = pd.read_csv(datapath + 'ratings.dat', delimiter='::', engine='python', header=None, names=['UserID', 'MovieID', 'Rating', 'Timestamp'], encoding='latin-1')\n",
    "\n",
    "# Load Users\n",
    "users = pd.read_csv(datapath + 'users.dat', delimiter='::', engine='python', header=None, names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c874124",
   "metadata": {},
   "source": [
    "Content-Based Filtering: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0e2e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050            Aladdin and the King of Thieves (1996)\n",
       "2072                          American Tail, An (1986)\n",
       "2073        American Tail: Fievel Goes West, An (1991)\n",
       "2285                         Rugrats Movie, The (1998)\n",
       "2286                              Bug's Life, A (1998)\n",
       "3045                                Toy Story 2 (1999)\n",
       "3542                             Saludos Amigos (1943)\n",
       "3682                                Chicken Run (2000)\n",
       "3685    Adventures of Rocky and Bullwinkle, The (2000)\n",
       "12                                        Balto (1995)\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TF-IDF matrix of unigrams, bigrams, and trigrams for each movie's genre\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies['Genres'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to get recommendations based on the cosine similarity score of movie genres\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = movies[movies['Title'] == title].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]  # 10 most similar movies\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return movies['Title'].iloc[movie_indices]\n",
    "\n",
    "# Example usage\n",
    "recommendations = get_recommendations('Toy Story (1995)')\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb50f01",
   "metadata": {},
   "source": [
    "Item-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68334a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33                                   Babe (1995)\n",
       "38                               Clueless (1995)\n",
       "257    Star Wars: Episode IV - A New Hope (1977)\n",
       "293                          Pulp Fiction (1994)\n",
       "315             Shawshank Redemption, The (1994)\n",
       "352                          Forrest Gump (1994)\n",
       "360                        Lion King, The (1994)\n",
       "453                         Fugitive, The (1993)\n",
       "476                         Jurassic Park (1993)\n",
       "584                               Aladdin (1992)\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pivot table with movies as rows and users as columns\n",
    "movie_ratings = ratings.pivot_table(index='MovieID', columns='UserID', values='Rating').fillna(0)\n",
    "item_similarity = cosine_similarity(movie_ratings)\n",
    "\n",
    "# Function to recommend movies based on item similarity\n",
    "def get_item_based_recommendation(movie_id):\n",
    "    # Get movie index for similarity matrix\n",
    "    idx = movies[movies['MovieID'] == movie_id].index[0]\n",
    "    similar_scores = item_similarity[idx]\n",
    "    similar_movies = list(movie_ratings.index[np.where(similar_scores > 0.5)])\n",
    "    similar_movies.remove(movie_id)  # Remove the movie itself from the recommendation\n",
    "    return movies[movies['MovieID'].isin(similar_movies)]['Title']\n",
    "\n",
    "# Example usage\n",
    "recommendations = get_item_based_recommendation(1)  # For movie with MovieID 1\n",
    "recommendations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b2dcd",
   "metadata": {},
   "source": [
    "User-Based Collaborative Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd687735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Toy Story (1995)',\n",
       " 'Pocahontas (1995)',\n",
       " 'Apollo 13 (1995)',\n",
       " 'Star Wars: Episode IV - A New Hope (1977)',\n",
       " \"Schindler's List (1993)\",\n",
       " 'Secret Garden, The (1993)',\n",
       " 'Aladdin (1992)',\n",
       " 'Snow White and the Seven Dwarfs (1937)',\n",
       " 'Beauty and the Beast (1991)',\n",
       " 'Fargo (1996)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pivot table with users as rows and movies as columns\n",
    "user_ratings = ratings.pivot_table(index='UserID', columns='MovieID', values='Rating').fillna(0)\n",
    "user_similarity = cosine_similarity(user_ratings)\n",
    "\n",
    "# Function to recommend movies based on user similarity\n",
    "def get_user_based_recommendation(user_id):\n",
    "    # Get user index for similarity matrix\n",
    "    idx = users[users['UserID'] == user_id].index[0]\n",
    "    similar_users = user_similarity[idx]\n",
    "    similar_users_index = np.where(similar_users > 0.5)[0]\n",
    "    recommended_movies = set()\n",
    "    for i in similar_users_index:\n",
    "        movies_rated_by_similar_user = user_ratings.columns[np.where(user_ratings.iloc[i] > 3)].tolist()\n",
    "        recommended_movies.update(movies_rated_by_similar_user)\n",
    "    return movies[movies['MovieID'].isin(recommended_movies)]['Title']\n",
    "\n",
    "# Example usage\n",
    "recommendations = get_user_based_recommendation(1)  # For user with UserID 1\n",
    "list(recommendations)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fa570",
   "metadata": {},
   "source": [
    "### (2)\n",
    "\n",
    "Deep learning-based algorithms\n",
    "\n",
    "Neural Collaborative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b65b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.users = ratings['UserID'].values\n",
    "        self.items = ratings['MovieID'].values\n",
    "        self.ratings = ratings['Rating'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'item': self.items[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size=50, hidden_layers=[100, 50], dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "\n",
    "        layers = []\n",
    "        input_size = embedding_size * 2  # User and item embeddings concatenated\n",
    "        for layer_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_size = layer_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedding = self.user_embedding(user_input)\n",
    "        item_embedding = self.item_embedding(item_input)\n",
    "        concatenated = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "        x = self.hidden_layers(concatenated)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47020c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max User ID: 6039, Max Movie ID: 3951\n"
     ]
    }
   ],
   "source": [
    "ratings['UserID'] = ratings['UserID'] - 1\n",
    "ratings['MovieID'] = ratings['MovieID'] - 1\n",
    "\n",
    "max_user_id = ratings['UserID'].max()\n",
    "max_movie_id = ratings['MovieID'].max()\n",
    "print(f'Max User ID: {max_user_id}, Max Movie ID: {max_movie_id}')\n",
    "\n",
    "dataset = MovieLensDataset(ratings)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = NCF(max_user_id + 1, max_movie_id + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144c57a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.0653433799743652\n",
      "Epoch 2/5, Loss: 0.6517144441604614\n",
      "Epoch 3/5, Loss: 0.7861999273300171\n",
      "Epoch 4/5, Loss: 1.0934501886367798\n",
      "Epoch 5/5, Loss: 0.6505059003829956\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        users = batch['user']\n",
    "        items = batch['item']\n",
    "        ratings = batch['rating'].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8dac21-e417-4c39-83df-2e46a425f0f3",
   "metadata": {},
   "source": [
    "GNN-based recommendation algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e9672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(datapath + 'ratings.dat', delimiter='::', engine='python', header=None, names=['UserID', 'MovieID', 'Rating', 'Timestamp'], encoding='latin-1')\n",
    "\n",
    "ratings['UserID'] = ratings['UserID'] - 1\n",
    "ratings['MovieID'] = ratings['MovieID'] - 1\n",
    "\n",
    "max_user_id = ratings['UserID'].max()\n",
    "max_movie_id = ratings['MovieID'].max()\n",
    "\n",
    "def create_adjacency_matrix(ratings, num_users, num_items):\n",
    "    user_item_matrix = sp.coo_matrix((ratings['Rating'], (ratings['UserID'], ratings['MovieID'] + num_users)), shape=(num_users + num_items, num_users + num_items), dtype=np.float32)\n",
    "    adj_matrix = user_item_matrix + user_item_matrix.T\n",
    "    adj_matrix = adj_matrix + sp.eye(adj_matrix.shape[0])\n",
    "    return adj_matrix\n",
    "\n",
    "adj_matrix = create_adjacency_matrix(ratings, max_user_id + 1, max_movie_id + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3bb5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, adjacency_matrix, features):\n",
    "        support = self.linear(features)\n",
    "        output = torch.spmm(adjacency_matrix, support)\n",
    "        return output\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, in_features, hidden_size=64):\n",
    "        super(GCN, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, in_features)\n",
    "        self.item_embedding = nn.Embedding(num_items, in_features)\n",
    "        self.gcn1 = GCNLayer(in_features, hidden_size)\n",
    "        self.gcn2 = GCNLayer(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, user_indices, item_indices, adjacency_matrix):\n",
    "        num_users, num_items = self.user_embedding.num_embeddings, self.item_embedding.num_embeddings\n",
    "        user_embedded = self.user_embedding(user_indices)\n",
    "        item_embedded = self.item_embedding(item_indices)\n",
    "        features = torch.zeros(num_users + num_items, user_embedded.size(1))\n",
    "        features.scatter_(0, user_indices.unsqueeze(1).repeat(1, user_embedded.size(1)), user_embedded)\n",
    "        features.scatter_(0, (num_users + item_indices).unsqueeze(1).repeat(1, item_embedded.size(1)), item_embedded)\n",
    "\n",
    "        hidden = F.relu(self.gcn1(adjacency_matrix, features))\n",
    "        output = F.relu(self.gcn2(adjacency_matrix, hidden))\n",
    "        user_output, item_output = output.split([num_users, num_items], 0)\n",
    "        user_output = user_output[user_indices]\n",
    "        item_output = item_output[item_indices]\n",
    "        interaction = torch.mul(user_output, item_output)\n",
    "        rating = self.fc(interaction)\n",
    "        return rating.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ac35b-4efa-4885-a725-3b2db2827f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 972168256.0\n"
     ]
    }
   ],
   "source": [
    "dataset = MovieLensDataset(ratings)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "embedding_size = 50\n",
    "gcn_model = GCN(max_user_id + 1, max_movie_id + 1, embedding_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gcn_model.parameters(), lr=0.001)\n",
    "\n",
    "adj_matrix_tensor = torch.FloatTensor(adj_matrix.toarray())\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        users = batch['user']\n",
    "        items = batch['item']\n",
    "        ratings = batch['rating'].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = gcn_model(users, items, adj_matrix_tensor).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93dbab-6856-4aec-b192-a35cbbfbc334",
   "metadata": {},
   "source": [
    "### (3)\n",
    "\n",
    "Knowledge Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81226a5-0852-4b4b-b790-2d39c1c8cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# MovieLens数据集路径\n",
    "datapath = \"data/ml-1m/\"\n",
    "\n",
    "# 读取电影数据\n",
    "movies = pd.read_csv(datapath + 'movies.dat', delimiter='::', engine='python', header=None, names=['MovieID', 'Title', 'Genres'], encoding='latin-1')\n",
    "movies = movies[:10]\n",
    "\n",
    "# 设置SPARQL\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# 清理电影标题（去除年份）\n",
    "def clean_title(title):\n",
    "    if '(' in title and ')' in title:\n",
    "        return title.split('(')[0].strip()\n",
    "    return title\n",
    "\n",
    "# 获取DBpedia URI\n",
    "def get_dbpedia_uri(movie_title):\n",
    "    cleaned_title = clean_title(movie_title)\n",
    "    query = \"\"\"\n",
    "    SELECT ?film WHERE {\n",
    "        ?film a dbo:Film .\n",
    "        ?film foaf:name '\"\"\" + cleaned_title.replace(\"'\", r\"\\'\") + \"\"\"'@en .\n",
    "    }\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    results = sparql.query().convert()\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        return results[\"results\"][\"bindings\"][0][\"film\"][\"value\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 获取电影特征\n",
    "def get_movie_features(dbpedia_uri):\n",
    "    if dbpedia_uri is None:\n",
    "        return {}\n",
    "    query = \"\"\"\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    SELECT ?director ?abstract ?releaseDate ?starring ?language WHERE {\n",
    "      <\"\"\" + dbpedia_uri + \"\"\"> dbo:director ?director .\n",
    "      <\"\"\" + dbpedia_uri + \"\"\"> dbo:abstract ?abstract .\n",
    "      OPTIONAL { <\"\"\" + dbpedia_uri + \"\"\"> dbo:releaseDate ?releaseDate . }\n",
    "      OPTIONAL { <\"\"\" + dbpedia_uri + \"\"\"> dbo:starring ?starring . }\n",
    "      OPTIONAL { <\"\"\" + dbpedia_uri + \"\"\"> dbo:language ?language . }\n",
    "      FILTER (lang(?abstract) = 'en')\n",
    "    }\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    results = sparql.query().convert()\n",
    "    features = {}\n",
    "    if results[\"results\"][\"bindings\"]:\n",
    "        binding = results[\"results\"][\"bindings\"][0]\n",
    "        features = {\n",
    "            \"director\": binding.get(\"director\", {}).get(\"value\"),\n",
    "            \"abstract\": binding.get(\"abstract\", {}).get(\"value\"),\n",
    "            \"releaseDate\": binding.get(\"releaseDate\", {}).get(\"value\"),\n",
    "            \"starring\": [b[\"starring\"][\"value\"] for b in results[\"results\"][\"bindings\"] if \"starring\" in b],\n",
    "            \"language\": binding.get(\"language\", {}).get(\"value\")\n",
    "        }\n",
    "    return features\n",
    "\n",
    "\n",
    "# 新增列用于存储特征\n",
    "movies['DBpedia_Director'] = None\n",
    "movies['DBpedia_Abstract'] = None\n",
    "movies['DBpedia_ReleaseDate'] = None\n",
    "movies['DBpedia_Starring'] = None\n",
    "movies['DBpedia_Language'] = None\n",
    "\n",
    "# 遍历电影数据集\n",
    "for index, row in movies.iterrows():\n",
    "    movie_title = row['Title']\n",
    "    dbpedia_uri = get_dbpedia_uri(movie_title)\n",
    "    movie_features = get_movie_features(dbpedia_uri)\n",
    "    movies.at[index, 'DBpedia_Director'] = movie_features.get('director')\n",
    "    movies.at[index, 'DBpedia_Abstract'] = movie_features.get('abstract')\n",
    "    movies.at[index, 'DBpedia_ReleaseDate'] = movie_features.get('releaseDate')\n",
    "    movies.at[index, 'DBpedia_Starring'] = movie_features.get('starring')\n",
    "    movies.at[index, 'DBpedia_Language'] = movie_features.get('language')\n",
    "\n",
    "# 显示更新后的DataFrame\n",
    "print(movies.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
